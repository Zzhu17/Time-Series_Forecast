import sys
import importlib
import pandas as pd
import numpy as np
from typing import Optional, Dict
import streamlit as st

# ---- Module-level constants ----
PLOT_MOD = "visualizations.plot"
MIME_CSV = "text/csv"

from services.pipeline import run_train_predict_pipeline
from types import SimpleNamespace
from models.informer.predict import InformerPredictor

# --------- always load latest plot module (Patch C) ---------
def load_plot_module():
    """
    Always return a fresh visualizations.plot module.
    Streamlit reruns can keep module cache; we reload explicitly.
    """
    if PLOT_MOD in sys.modules:
        importlib.reload(sys.modules[PLOT_MOD])
        return sys.modules[PLOT_MOD]
    else:
        mod = importlib.import_module(PLOT_MOD)
        return mod

# ---- safe helpers ----
def _as_int(x, default: Optional[int] = None) -> Optional[int]:
    """Best-effort convert to int; return default on failure."""
    try:
        if isinstance(x, (int, np.integer)):
            return int(x)
        if isinstance(x, str):
            xs = x.strip()
            if xs.isdigit() or (xs.startswith('-') and xs[1:].isdigit()):
                return int(xs)
            return int(float(xs))
        if x is not None:
            return int(x)
    except Exception:
        pass
    return default

# ---- build DataFrame from long JSON safely ----
def _df_from_long(long_obj, time_col: str, value_name_true: str = 'y_true', value_name_pred: str = 'yhat') -> pd.DataFrame:
    try:
        if not isinstance(long_obj, dict):
            return pd.DataFrame(columns=[time_col, value_name_true, value_name_pred])
        ts  = list(long_obj.get('timestamps') or [])
        y_t = list(long_obj.get('y_true') or [])
        y_h = list(long_obj.get('yhat') or [])
        # align lengths
        n = min(len(ts), len(y_t), len(y_h))
        if n == 0:
            return pd.DataFrame(columns=[time_col, value_name_true, value_name_pred])
        df = pd.DataFrame({
            time_col: ts[:n],
            value_name_true: pd.to_numeric(y_t[:n], errors='coerce'),
            value_name_pred: pd.to_numeric(y_h[:n], errors='coerce'),
        })
        return df
    except Exception:
        return pd.DataFrame(columns=[time_col, value_name_true, value_name_pred])

# æ–°å¢ï¼šå°† dense DataFrame è½¬ä¸ºæ ‡å‡†æ˜ç»† DataFrame ç”¨äºå±•ç¤º/å¯¼å‡º
def _df_from_dense_for_display(df_dense: Optional[pd.DataFrame], time_col: str) -> pd.DataFrame:
    if not isinstance(df_dense, pd.DataFrame) or df_dense.empty:
        return pd.DataFrame(columns=[time_col, "y_true", "yhat"])
    out = df_dense.copy()
    # å¦‚æœæ˜¯ DatetimeIndexï¼Œåˆ™ reset_index æˆæ—¶é—´åˆ—
    if isinstance(out.index, pd.DatetimeIndex):
        out = out.reset_index().rename(columns={out.index.name or "index": time_col})
    # åªä¿ç•™æ ‡å‡†åˆ—
    keep = [c for c in [time_col, "y_true", "yhat"] if c in out.columns]
    return out[keep]


# ==========================
# ======= Streamlit UI =====
# ==========================

st.set_page_config(page_title="Universal TS Forecast", layout="wide")
st.title("ğŸ§  é€šç”¨æ—¶é—´åºåˆ—é¢„æµ‹å¹³å°ï¼ˆä¸­æ§å°ç®€çº¦ç‰ˆï¼‰")

# ä¸»åŒºåŸŸï¼šä¸Šä¼ æ–‡ä»¶ + é€‰æ‹©æ¨¡å‹ + è¿è¡Œ
uploaded = st.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶", type=["csv"])
model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["informer", "arima", "prophet", "randomforest", "lstm"], index=0)
if model_name == "randomforest":
    st.caption("âš™ï¸ RandomForest å°†å¼ºåˆ¶è¿›è¡Œ Optuna è°ƒå‚ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼Œn_trials ç”± configs.yaml çš„ optimization.n_trials æ§åˆ¶ï¼Œé»˜è®¤ 50ï¼‰ã€‚")
if model_name == "lstm":
    st.caption("ğŸ§© LSTM å°†ä½¿ç”¨ configs.yaml çš„ model_config.LSTM è¶…å‚ï¼ˆseq_len/hidden_dim/num_layers/n_epochs/learning_rateï¼‰ï¼Œå¹¶é€šè¿‡é€‚é…å±‚ç”Ÿæˆæ•´æ®µ val/test é¢„æµ‹ã€‚")
run_click = st.button("å¼€å§‹è®­ç»ƒå¹¶é¢„æµ‹", type="primary")

# åœ¨çº¿æ»šåŠ¨æ¨ç†å‚æ•°ï¼ˆä¸é‡æ–°è®­ç»ƒï¼‰
col_r1, col_r2, col_r3 = st.columns([1,1,2])
with col_r1:
    horizon_days = st.selectbox("åœ¨çº¿é¢„æµ‹åœ°å¹³çº¿ï¼ˆå¤©ï¼‰", [1, 3, 7], index=0)
with col_r2:
    step_mode = st.selectbox("æ»šåŠ¨æ­¥å¹…", ["å—æ¨è¿›(=åœ°å¹³çº¿)", "é€æ­¥æ¨è¿›(=1)"], index=0)
with col_r3:
    st.caption("å—æ¨è¿›é€Ÿåº¦å¿«ã€è¯¯å·®ä¸ç´¯ç§¯ï¼›é€æ­¥æ›´å¹³æ»‘ä½†æ›´æ…¢ä¸”è¯¯å·®é€’æ¨ã€‚")

online_click = st.button("ä»…é¢„æµ‹ï¼ˆåœ¨çº¿æ»šåŠ¨æ¨ç†ï¼‰", type="secondary")

if uploaded is None:
    st.info("è¯·å…ˆä¸Šä¼  CSV æ–‡ä»¶ã€‚")
else:
    # è¯»å–æ•°æ®å¹¶åšåŸºæœ¬æ ¡éªŒ
    try:
        df = pd.read_csv(uploaded)
    except Exception as e:
        st.error(f"è¯»å– CSV å¤±è´¥ï¼š{e}")
        st.stop()

    # ç®€å•çš„åˆ—åæ¨æ–­ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™ç»™å‡ºæç¤ºï¼‰
    time_col = 'date' if 'date' in df.columns else df.columns[0]
    if 'value' in df.columns:
        value_col = 'value'
    elif len(df.columns) > 1:
        value_col = df.columns[1]
    else:
        value_col = df.columns[0]

    # === è‡ªåŠ¨æ¨æ–­ feature_colsï¼ˆå•/å¤šå˜é‡è‡ªé€‚é…ï¼‰===
    numeric_cols = [c for c in df.select_dtypes(include='number').columns if c != time_col]
    feature_cols = [value_col] + [c for c in numeric_cols if c != value_col]

    missing_cols = [c for c in (time_col, value_col) if c not in df.columns]
    if missing_cols:
        st.error(f"CSV ä¸­ç¼ºå°‘å¿…è¦åˆ—ï¼š{missing_cols}")
        st.stop()

    st.subheader("ğŸ“„ æ•°æ®æ¦‚è§ˆ")
    st.caption(f"æ—¶é—´åˆ—: {time_col} | ç›®æ ‡åˆ—: {value_col}")
    st.dataframe(df.head(10), use_container_width=True)

    # ==========================
    # åœ¨çº¿æ»šåŠ¨æ¨ç†ï¼ˆä¸è®­ç»ƒï¼Œç›´æ¥åŠ è½½æƒé‡å¹¶æŒ‰æ•´æ®µæ»šåŠ¨é¢„æµ‹ï¼‰
    # ==========================
    if online_click:
        config_pred = {
            "default": {
                "time_col": time_col,
                "value_col": value_col,
                "device": "cpu",
                "dtype": "float32",
            },
            "model_config": {
                "Informer": {
                    "seq_len": 96,
                    "label_len": 48,
                    "pred_len": 24,  # å®é™…æ»šåŠ¨åœ°å¹³çº¿ç”± horizon è¦†ç›–
                    "feature_cols": feature_cols,
                }
            },
            "artifacts": {
                "model_path": "artifacts/informer_model.pth",
                "scaler_path": "artifacts/scaler.pkl",
                "residual_model_path": "artifacts/residual_model.pkl",
            },
            "prediction": {
                "rolling": {
                    "enabled": True,
                    "step": None,
                    "mode": "overwrite",
                }
            }
        }

        st.caption(f"ä½¿ç”¨ç‰¹å¾åˆ—ï¼ˆæŒ‰è®­ç»ƒ/é¢„æµ‹å›ºå®šé¡ºåºï¼‰ï¼š{feature_cols}")

        # åœ°å¹³çº¿ä¸æ­¥å¹…
        horizon_steps = int(24 * horizon_days)  # è‹¥æ˜¯æ—¥é¢‘è‡ªè¡Œè°ƒæ•´å€æ•°
        step_val = None if step_mode.startswith("å—") else 1

        try:
            predictor = InformerPredictor(config_pred)
        except Exception as e:
            st.error("åŠ è½½å·²è®­ç»ƒæ¨¡å‹å¤±è´¥ï¼ˆè¯·å…ˆå®Œæˆä¸€æ¬¡è®­ç»ƒæˆ–æ£€æŸ¥ artifacts è·¯å¾„ï¼‰")
            st.exception(e)
            st.stop()

        with st.spinner("åœ¨çº¿æ»šåŠ¨æ¨ç†ä¸­..."):
            try:
                merged = predictor.rolling_predict(df.copy(), horizon=horizon_steps, step=step_val, mode="overwrite")
            except Exception as e:
                st.error("åœ¨çº¿æ»šåŠ¨æ¨ç†å¤±è´¥")
                st.exception(e)
                st.stop()

        # è®¡ç®—ä¸çœŸå€¼é‡å åŒºé—´çš„æŒ‡æ ‡
        merged = np.asarray(merged).reshape(-1)
        mask = ~np.isnan(merged)
        if mask.sum() == 0:
            st.warning("æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆçš„é¢„æµ‹åŒºé—´ï¼ˆæ•°æ®å¤ªçŸ­æˆ–å‚æ•°ä¸åŒ¹é…ï¼‰")
        else:
            y_true = pd.to_numeric(df.loc[mask, value_col], errors='coerce').to_numpy()
            y_hat = merged[mask]
            rmse = float(np.sqrt(np.nanmean((y_hat - y_true) ** 2)))
            denom = np.where(y_true == 0, np.nan, np.abs(y_true))
            mape = float(np.nanmean(np.abs((y_hat - y_true) / denom)) * 100.0)

            st.subheader("âš¡ åœ¨çº¿æ»šåŠ¨æ¨ç† â€” æŒ‡æ ‡")
            c1, c2 = st.columns(2)
            with c1:
                st.metric("Online RMSE", f"{rmse:.4f}")
            with c2:
                st.metric("Online MAPE", f"{mape:.2f}%")

            # åœ¨çº¿é•¿åºåˆ—
            online_long = {
                "timestamps": pd.to_datetime(df[time_col]).astype(str).tolist(),
                "y_true": pd.to_numeric(df[value_col], errors='coerce').astype(float).tolist(),
                "yhat": merged.astype(float).tolist(),
            }

            st.subheader("ğŸ“ˆ åœ¨çº¿æ»šåŠ¨æ¨ç† â€” é•¿åºåˆ—æ›²çº¿")
            vplot = load_plot_module()
            fig_online = vplot.plot_results(
                train_df=df[[time_col, value_col]] if time_col in df.columns and value_col in df.columns else pd.DataFrame(columns=[value_col]),
                val_df_aligned=None,
                test_df_aligned=None,
                time_col=time_col,
                value_col=value_col,
                title=f"Online Rolling Inference (H={horizon_steps}, step={'H' if step_val is None else step_val})",
                payload=None,
                val_long=online_long,
                test_long=None,
                train_len=None,
                val_len=None,
                test_len=None,
            )
            st.pyplot(fig_online)

            with st.expander("ğŸ” åœ¨çº¿æ»šåŠ¨æ¨ç†æ˜ç»†ï¼ˆæœ€è¿‘ 200 æ¡ï¼‰", expanded=False):
                view_df = pd.DataFrame({
                    time_col: online_long["timestamps"],
                    "y_true": online_long["y_true"],
                    "yhat": online_long["yhat"],
                }).tail(200)
                st.dataframe(view_df, use_container_width=True)

            with st.expander("ğŸ§¾ åœ¨çº¿æ»šåŠ¨æ¨ç†æ˜ç»†ï¼ˆæ•´æ®µï¼‰", expanded=False):
                full_df = _df_from_long(online_long, time_col)
                st.dataframe(full_df, use_container_width=True)
                try:
                    st.download_button(
                        label="ä¸‹è½½åœ¨çº¿æ•´æ®µæ˜ç»† CSV",
                        data=full_df.to_csv(index=False).encode('utf-8'),
                        file_name="online_long.csv",
                        mime=MIME_CSV,
                    )
                except Exception:
                    pass
    # ==========================
    # è®­ç»ƒ + é¢„æµ‹ + ç»Ÿä¸€ç»˜å›¾ï¼ˆ6/2/2 + é•¿åºåˆ—ï¼‰
    # ==========================
    if run_click:
        # è®­ç»ƒ + é¢„æµ‹ + ç»Ÿä¸€ç»˜å›¾ï¼ˆ6/2/2 + é•¿åºåˆ—ï¼‰
        config = {
            "model": {"name": model_name},
            "default": {
                "time_col": time_col,
                "value_col": value_col,
            },
            "model_config": {
                "Informer": {
                    "seq_len": 96,
                    "label_len": 48,
                    "pred_len": 24,
                    "feature_cols": feature_cols,
                }
            },
            "artifacts": {
                "model_path": "artifacts/informer_model.pth",
                "scaler_path": "artifacts/scaler.pkl",
                "residual_model_path": "artifacts/residual_model.pkl",
                "y_scaler_path": "artifacts/value_scaler.pkl",
            }
        }
        # ä¿è¯æ–°æ—§é…ç½®é”®éƒ½èƒ½è¢« pipeline è¯†åˆ«ï¼ˆæ”¾åœ¨å®šä¹‰ config ä¹‹åï¼‰
        config.setdefault("model", {})["name"] = model_name
        config["model_type"] = model_name

        # ä¸ºæ³¨å†Œè¡¨æ¨¡å‹ï¼ˆå¦‚ arimaï¼‰æä¾›åŸå§‹ DataFrameï¼ˆpipeline ä¼šä¼˜å…ˆè¯»å–è¿™é‡Œï¼‰
        config.setdefault("data", {})
        config["data"]["dataframe"] = df.copy()

        def _prepare_data_into_config(src_df, cfg, feature_cols):
            """Prepare cfg['data'] for single-arg pipeline: 6:2:2 split + scaler fit/transform."""
            import numpy as _np
            import pandas as _pd

            # Robust StandardScaler import with a minimal fallback implementation
            try:
                from sklearn.preprocessing import StandardScaler as _RealStandardScaler
                SSCls = _RealStandardScaler
            except Exception:
                class _MiniStandardScaler:
                    def fit(self, X):
                        X = _np.asarray(X, dtype=_np.float32)
                        self.mean_ = X.mean(axis=0)
                        self.scale_ = X.std(axis=0)
                        self.scale_[self.scale_ == 0] = 1.0
                        self.n_features_in_ = X.shape[1]
                        return self
                    def transform(self, X):
                        X = _np.asarray(X, dtype=_np.float32)
                        return (X - self.mean_) / self.scale_
                    def inverse_transform(self, X):
                        X = _np.asarray(X, dtype=_np.float32)
                        return X * self.scale_ + self.mean_
                SSCls = _MiniStandardScaler

            time_col = cfg.get('default', {}).get('time_col', 'date')
            value_col = cfg.get('default', {}).get('value_col', 'value')

            df2 = src_df.copy()
            # ensure sorting by time if time_col exists
            if time_col in df2.columns:
                try:
                    df2[time_col] = _pd.to_datetime(df2[time_col])
                    df2 = df2.sort_values(time_col)
                except Exception:
                    pass

            n = len(df2)
            t = int(n * 0.6); v = int(n * 0.2); te = n - t - v
            train_df = df2.iloc[:t].copy()
            val_df   = df2.iloc[t:t+v].copy()
            test_df  = df2.iloc[t+v:].copy()

            # guard: keep only existing feature columns
            feat_cols = [c for c in list(feature_cols) if c in df2.columns]

            # fit scaler on train
            scaler = SSCls()
            if len(feat_cols) == 0:
                raise ValueError("No valid feature columns found for scaling.")
            scaler.fit(train_df[feat_cols].astype('float32'))

            # transform helper
            def _tf(d):
                out = d.copy()
                out[feat_cols] = scaler.transform(d[feat_cols].astype('float32'))
                return out

            cfg.setdefault('data', {})
            cfg['data']['train_df_sc'] = _tf(train_df)
            cfg['data']['val_df_sc']   = _tf(val_df)
            cfg['data']['test_df_sc']  = _tf(test_df)
            cfg['data']['split'] = {"train_len": t, "val_len": v, "test_len": te}
            cfg['data']['all_feature_cols'] = list(feat_cols)
            cfg.setdefault('artifacts', {})['scaler'] = scaler

        def _normalize_results(res, cfg, src_df):
            """ç»Ÿä¸€æŠŠ pipeline è¿”å›å€¼è§„æ•´ä¸º {'status','metrics','data','artifacts'} ç»“æ„ã€‚æ”¯æŒ:
            - (model, result_df)
            - {'status': 'ok', 'metrics': ..., 'data': {...}}
            å¹¶ä» cfg['data'] ä¸­æå– val_long/test_long/split ä¿¡æ¯ã€‚
            """
            out = {"status": "ok", "metrics": {}, "data": {}, "artifacts": cfg.get("artifacts", {})}
            data_blk = cfg.get("data", {}) or {}

            def _extract_metrics_from_cfg(_d: dict) -> dict:
                if not isinstance(_d, dict):
                    return {}
                out_m = {}
                # validation candidates
                for k in ("metrics_val", "val_metrics", "validation_metrics", "metrics_validation"):
                    vm = _d.get(k)
                    if isinstance(vm, dict) and vm:
                        out_m["validation"] = vm
                        break
                # test candidates
                for k in ("metrics_test", "test_metrics", "testing_metrics", "metrics_testing"):
                    tm = _d.get(k)
                    if isinstance(tm, dict) and tm:
                        out_m["test"] = tm
                        break
                return out_m

            def _extract_metrics_from_root(_cfg: dict) -> dict:
                """
                æ”¯æŒä» cfg['metrics'] è¯»å–æ‰å¹³é”®ï¼šval_rmse/val_mape/test_rmse/test_mape
                å¹¶æ˜ å°„ä¸º {'validation': {'rmse','mape'}, 'test': {'rmse','mape'}}
                """
                if not isinstance(_cfg, dict):
                    return {}
                root_m = _cfg.get("metrics") or {}
                if not isinstance(root_m, dict):
                    return {}
                out_m = {}
                val_m = {}
                test_m = {}
                # validation
                if "val_rmse" in root_m: val_m["rmse"] = root_m.get("val_rmse")
                if "val_mape" in root_m: val_m["mape"] = root_m.get("val_mape")
                if "val_mape_safe" in root_m: val_m["mape_safe"] = root_m.get("val_mape_safe")
                if val_m:
                    out_m["validation"] = val_m
                # test
                if "test_rmse" in root_m: test_m["rmse"] = root_m.get("test_rmse")
                if "test_mape" in root_m: test_m["mape"] = root_m.get("test_mape")
                if "test_mape_safe" in root_m: test_m["mape_safe"] = root_m.get("test_mape_safe")
                if test_m:
                    out_m["test"] = test_m
                return out_m

            # 1) æ ‡å‡† dict è¿”å›
            if isinstance(res, dict):
                out.update(res)
                out.setdefault("data", {})

                # é€ä¼ é•¿è½½è·ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
                if "val_long" not in out["data"] and "val_long" in data_blk:
                    out["data"]["val_long"] = data_blk.get("val_long")
                if "test_long" not in out["data"] and "test_long" in data_blk:
                    out["data"]["test_long"] = data_blk.get("test_long")

                # âœ… æ–°å¢ï¼šé€ä¼  denseï¼ˆå¯¹é½å¥½çš„æ•´æ®µ DataFrameï¼‰
                if "val_dense" not in out["data"] and "val_dense" in data_blk:
                    out["data"]["val_dense"] = data_blk.get("val_dense")
                if "test_dense" not in out["data"] and "test_dense" in data_blk:
                    out["data"]["test_dense"] = data_blk.get("test_dense")

                # split ä¿¡æ¯å…œåº•
                if "split" not in out["data"]:
                    n = len(src_df)
                    t = int(n * 0.6)
                    v = int(n * 0.2)
                    out["data"]["split"] = {"train_len": t, "val_len": v, "test_len": n - t - v}

                # backfill metrics from cfg['data'] if missing/partial
                cfg_metrics = _extract_metrics_from_cfg(data_blk)
                if cfg_metrics:
                    out.setdefault("metrics", {})
                    # don't overwrite existing sections
                    for sect, md in cfg_metrics.items():
                        if sect not in out["metrics"] or not out["metrics"][sect]:
                            out["metrics"][sect] = md
                # æ–°å¢ï¼šä» cfg['metrics'] æ‰å¹³é”®æå–
                root_metrics = _extract_metrics_from_root(cfg)
                if root_metrics:
                    out.setdefault("metrics", {})
                    for sect, md in root_metrics.items():
                        if sect not in out["metrics"] or not out["metrics"][sect]:
                            out["metrics"][sect] = md
                return out

            # 2) äºŒå…ƒç»„è¿”å›ï¼š (model, result_df)
            if isinstance(res, (tuple, list)) and len(res) >= 1:
                # ä» cfg['data'] é‡Œæ‹¿è½½è·
                out["data"]["val_long"]  = data_blk.get("val_long")
                out["data"]["test_long"] = data_blk.get("test_long")
                # âœ… æ–°å¢ï¼šdense
                out["data"]["val_dense"] = data_blk.get("val_dense")
                out["data"]["test_dense"] = data_blk.get("test_dense")

                # split å…œåº•
                n = len(src_df)
                t = int(n * 0.6)
                v = int(n * 0.2)
                out["data"]["split"] = {"train_len": t, "val_len": v, "test_len": n - t - v}

                # also try to attach metrics from cfg['data']
                cfg_metrics = _extract_metrics_from_cfg(data_blk)
                if cfg_metrics:
                    out["metrics"] = cfg_metrics
                # æ–°å¢ï¼šä» cfg['metrics'] æ‰å¹³é”®æå–
                root_metrics = _extract_metrics_from_root(cfg)
                if root_metrics:
                    out.setdefault("metrics", {})
                    for sect, md in root_metrics.items():
                        if sect not in out["metrics"] or not out["metrics"][sect]:
                            out["metrics"][sect] = md
                return out

            # 3) å…œåº•
            cfg_metrics = _extract_metrics_from_cfg(data_blk)
            if cfg_metrics:
                out["metrics"] = cfg_metrics
            # æ–°å¢ï¼šä» cfg['metrics'] æ‰å¹³é”®æå–
            root_metrics = _extract_metrics_from_root(cfg)
            if root_metrics:
                out.setdefault("metrics", {})
                for sect, md in root_metrics.items():
                    if sect not in out["metrics"] or not out["metrics"][sect]:
                        out["metrics"][sect] = md
            out["status"] = "error"
            out["message"] = "Unknown pipeline return type"
            return out

        with st.spinner("è®­ç»ƒä¸é¢„æµ‹ä¸­ï¼Œè¯·ç¨å€™..."):
            try:
                # å…¼å®¹ä¸¤ç§ pipeline ç­¾åï¼š (df, config) æˆ– (config)
                import inspect  # local import to avoid top-level dependency
                sig = inspect.signature(run_train_predict_pipeline)
                params = list(sig.parameters.values())
                # å¦‚æœ pipeline åªæœ‰ä¸€ä¸ªå‚æ•°ï¼ˆconfigï¼‰ï¼Œå…ˆæŠŠæ•°æ®å†™å…¥ cfg['data']
                if len(params) < 2:
                    _prepare_data_into_config(df.copy(), config, feature_cols)
                # æ ¹æ® pipeline çš„ç­¾ååŠ¨æ€ç»„è£…å‚æ•°
                call_args = (df.copy(), config) if len(params) >= 2 else (config,)
                raw_results = run_train_predict_pipeline(*call_args)  # type: ignore[call-arg]
                results = _normalize_results(raw_results, config, df)
            except Exception as e:
                st.error("pipeline è¿è¡Œå¤±è´¥")
                st.exception(e)
                st.stop()

        status = results.get("status", "error")
        if status not in ("ok", "success"):
            st.error(results.get("message", "è®­ç»ƒ/é¢„æµ‹å¤±è´¥"))
            tb = results.get("traceback")
            if tb:
                st.code(tb)
            st.stop()

        # æŒ‡æ ‡å±•ç¤ºï¼ˆè‹¥æ²¡æœ‰åˆ™æ˜¾ç¤ºå ä½ï¼‰
        metrics = results.get("metrics", {}) or {}
        val_metrics = metrics.get("validation", {}) or {}
        test_metrics = metrics.get("test", {}) or {}

        # å…¼å®¹ï¼šå¦‚æœæ˜¯æ‰å¹³é”®ï¼ˆval_rmse/val_mape/test_rmse/test_mapeï¼‰ï¼Œè½¬æˆåˆ†ç»„ç»“æ„
        if (not val_metrics) and any(k in metrics for k in ("val_rmse", "val_mape", "val_mape_safe")):
            val_metrics = {
                "rmse": metrics.get("val_rmse"),
                "mape": metrics.get("val_mape"),
                "mape_safe": metrics.get("val_mape_safe"),
            }
        if (not test_metrics) and any(k in metrics for k in ("test_rmse", "test_mape", "test_mape_safe")):
            test_metrics = {
                "rmse": metrics.get("test_rmse"),
                "mape": metrics.get("test_mape"),
                "mape_safe": metrics.get("test_mape_safe"),
            }

        def _fmt(x, pct=False, safe=False, metrics=None):
            # å¦‚æœéœ€è¦ä» metrics é‡Œå–ï¼ˆæ¯”å¦‚ mape_safe ä¼˜å…ˆï¼‰ï¼Œåœ¨è¿™é‡Œæ›¿æ¢ x
            if safe and isinstance(metrics, dict):
                if metrics.get("mape_safe") is not None:
                    x = metrics.get("mape_safe")
                elif metrics.get("mape") is not None:
                    x = metrics.get("mape")
            if x is None:
                return "â€”"
            try:
                xv = float(x)
                if pct:
                    xv = xv * 100.0  # å°†æ¯”ä¾‹è½¬ä¸ºç™¾åˆ†æ•°
                    return f"{xv:.2f}%"
                return f"{xv:.4f}"
            except Exception:
                return str(x)

        c1, c2, c3, c4 = st.columns(4)
        with c1:
            st.metric("Val RMSE", _fmt(val_metrics.get("rmse")))
        with c2:
            st.metric("Val MAPE", _fmt(None, pct=True, safe=True, metrics=val_metrics))
        with c3:
            st.metric("Test RMSE", _fmt(test_metrics.get("rmse")))
        with c4:
            st.metric("Test MAPE", _fmt(None, pct=True, safe=True, metrics=test_metrics))

        # è‹¥ artifacts ä¸­åŒ…å« RF æœ€ä½³è¶…å‚ï¼Œåˆ™å•ç‹¬å±•ç¤º
        _arts = results.get("artifacts", {}) or {}
        _rf_params = _arts.get("randomforest_params") or _arts.get("best_params") or _arts.get("rf_best_params")
        if _rf_params and model_name == "randomforest":
            with st.expander("ğŸ§  RandomForest æœ€ä½³è¶…å‚ï¼ˆOptunaï¼‰", expanded=False):
                st.json(_rf_params)

        # åˆ‡åˆ†ä¿¡æ¯
        data_blob = results.get("data", {}) or {}
        split_info = data_blob.get("split", {}) or {}
        train_len = _as_int(split_info.get("train_len"))
        val_len = _as_int(split_info.get("val_len"))
        test_len = _as_int(split_info.get("test_len"))

        if train_len is not None and val_len is not None and test_len is not None:
            t, v, te = int(train_len), int(val_len), int(test_len)
            total = t + v + te
            if total > 0:
                st.caption(f"æ•°æ®åˆ‡åˆ†ï¼štrain={t}, val={v}, test={te}ï¼ˆæ¯”ä¾‹çº¦ä¸º {t/total:.2f}/{v/total:.2f}/{te/total:.2f} ï¼‰")
            else:
                st.caption(f"æ•°æ®åˆ‡åˆ†ï¼štrain={t}, val={v}, test={te}")

        # é•¿åºåˆ—è½½è·å’Œ dense è½½è·ï¼ˆå¦‚æœ‰ï¼‰
        val_long = data_blob.get("val_long")
        test_long = data_blob.get("test_long")
        val_dense = data_blob.get("val_dense") if "val_dense" in data_blob else None
        test_dense = data_blob.get("test_dense") if "test_dense" in data_blob else None

        # æ˜ç»†è¡¨ï¼ˆæ•´æ®µï¼‰ï¼šä¼˜å…ˆ dense DataFrameï¼Œå¦‚æœæœ‰
        def _coerce_dense(d):
            if d is None:
                return None
            try:
                if isinstance(d, pd.DataFrame):
                    return d
                return pd.DataFrame(d)
            except Exception:
                return None

        val_df_aligned = _coerce_dense(val_dense)
        test_df_aligned = _coerce_dense(test_dense)

        # å…¼å®¹ï¼šè‹¥ dense ä¸å¯ç”¨ï¼Œåˆ™å›é€€åˆ° longï¼ˆå­—å…¸ï¼‰å½¢å¼
        if val_df_aligned is None:
            val_long_df = _df_from_long(val_long, time_col)
        else:
            val_long_df = val_df_aligned.copy()
        if test_df_aligned is None:
            test_long_df = _df_from_long(test_long, time_col)
        else:
            test_long_df = test_df_aligned.copy()

        # è®­ç»ƒæ®µ DataFrameï¼ˆç”¨äºå››çº¿å›¾èƒŒæ™¯ï¼‰
        if train_len is not None and train_len > 0:
            _train_df_for_plot = df.iloc[:train_len][[time_col, value_col]]
        else:
            _train_df_for_plot = df[[time_col, value_col]]

        # ç»˜å›¾ï¼ˆæ•´æ®µï¼‰ã€‚æ³¨æ„ï¼šä¸å†ä¼ é€’ä¸å­˜åœ¨çš„å½¢å‚ val_dense/test_denseï¼›
        # å°† dense ä½œä¸ºå¯¹é½å¥½çš„ DataFrame ç›´æ¥é€šè¿‡ val_df_aligned/test_df_aligned ä¼ å…¥ã€‚
        st.subheader("ğŸ“ˆ é¢„æµ‹ç»“æœï¼ˆæ•´æ®µï¼‰")
        vplot = load_plot_module()
        try:
            # å°† dense ç›´æ¥ä½œä¸ºå¯¹é½å¥½çš„ DataFrame ä¼ å…¥
            fig = vplot.plot_results(
                train_df=_train_df_for_plot,
                val_df_aligned=val_dense if isinstance(val_dense, pd.DataFrame) and not val_dense.empty else None,
                test_df_aligned=test_dense if isinstance(test_dense, pd.DataFrame) and not test_dense.empty else None,
                time_col=time_col,
                value_col=value_col,
                title="Forecast Resultsï¼ˆæ•´æ®µï¼‰",
                payload=None,           # å¦‚éœ€é¢å¤–ä¿¡æ¯ï¼ˆsplitç­‰ï¼‰ä¹Ÿå¯ä»¥æ”¾åˆ° payload
                val_long=val_long,      # ä½œä¸ºåå¤‡å…œåº•ï¼ˆå½“å‰æˆ‘ä»¬å·²èµ° denseï¼Œä¸ä¼šç”¨åˆ°ï¼‰
                test_long=test_long,
                train_len=train_len,
                val_len=val_len,
                test_len=test_len,
            )
            st.pyplot(fig)
        except Exception as e:
            st.error("ç»˜å›¾å¤±è´¥")
            st.exception(e)

        # æ˜ç»†å¯¼å‡ºï¼ˆä¼˜å…ˆ denseï¼Œå…¶æ¬¡ longï¼‰
        with st.expander("ğŸ§¾ éªŒè¯é›†æ˜ç»†ï¼ˆæ•´æ®µï¼‰", expanded=False):
            val_long_df = _df_from_dense_for_display(val_dense, time_col) \
                          if isinstance(val_dense, pd.DataFrame) else _df_from_long(val_long, time_col)
            if not val_long_df.empty:
                st.dataframe(val_long_df, use_container_width=True)
                try:
                    st.download_button(
                        label="ä¸‹è½½éªŒè¯æ•´æ®µæ˜ç»† CSV",
                        data=val_long_df.to_csv(index=False).encode('utf-8'),
                        file_name="val_dense.csv" if isinstance(val_dense, pd.DataFrame) else "val_long.csv",
                        mime=MIME_CSV,
                    )
                except Exception:
                    pass
            else:
                st.info("æš‚æ— éªŒè¯æ•´æ®µæ˜ç»†ã€‚")

        with st.expander("ğŸ§¾ æµ‹è¯•é›†æ˜ç»†ï¼ˆæ•´æ®µï¼‰", expanded=False):
            test_long_df = _df_from_dense_for_display(test_dense, time_col) \
                           if isinstance(test_dense, pd.DataFrame) else _df_from_long(test_long, time_col)
            if not test_long_df.empty:
                st.dataframe(test_long_df, use_container_width=True)
                try:
                    st.download_button(
                        label="ä¸‹è½½æµ‹è¯•æ•´æ®µæ˜ç»† CSV",
                        data=test_long_df.to_csv(index=False).encode('utf-8'),
                        file_name="test_dense.csv" if isinstance(test_dense, pd.DataFrame) else "test_long.csv",
                        mime=MIME_CSV,
                    )
                except Exception:
                    pass
            else:
                st.info("æš‚æ— æµ‹è¯•æ•´æ®µæ˜ç»†ã€‚")

        # å·¥ä»¶è·¯å¾„å±•ç¤ºï¼ˆæœ‰å°±å±•ç¤ºï¼‰
        with st.expander("ğŸ§³ Artifacts", expanded=False):
            st.json(results.get("artifacts", {}))